{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2b1218-f752-4e49-b0f6-95f2ac8b8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loaded 78,324 rows from raw_cc_transactions.csv\n",
      "üßπ Dropped 1,114 rows with missing data\n",
      "‚úÖ Saved X_processed.npy and y.csv for downstream modeling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/30 21:14:14 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model 'CC Fraud Preprocessing' already exists. Creating a new version of this model...\n",
      "2025/06/30 21:14:18 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: CC Fraud Preprocessing, version 21\n",
      "Created version '21' of model 'CC Fraud Preprocessing'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506387e7883d48358f1630eb5bec2c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|‚ñå         | 1/19 [00:00<00:02,  7.21it/s]\u001b[A\n",
      " 11%|‚ñà         | 2/19 [00:00<00:02,  8.40it/s]\u001b[A\n",
      " 16%|‚ñà‚ñå        | 3/19 [00:00<00:01,  8.95it/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 34.51it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca295dbe19a844fabe52f433e6a164f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841403f356a74c63b4f0dacf709d2c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79510252c0b4cbbacbe515fe3fd9f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run Preprocessing Pipeline at: http://127.0.0.1:8768/#/experiments/1541/runs/6d98ac9891b7416eb6d3632cab576054\n",
      "üß™ View experiment at: http://127.0.0.1:8768/#/experiments/1541\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion, Processing, and MLflow Model Logging\n",
    "import io, os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# from domino.data_sources import DataSourceClient\n",
    "from domino_data.data_sources import DataSourceClient\n",
    "from domino_data.datasets import DatasetClient\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "domino_working_dir = os.environ.get(\"DOMINO_WORKING_DIR\", \".\")\n",
    "domino_datasource_dir = domino_working_dir.replace('code', 'data')\n",
    "domino_artifact_dir = domino_working_dir.replace('code', 'artifacts')\n",
    "domino_project_name = os.environ.get(\"DOMINO_PROJECT_NAME\", \"my-local-project\")\n",
    "\n",
    "## Data Ingestion, Processing, and MLflow Model Logging\n",
    "\n",
    "mlflow.set_experiment('CC Fraud Preprocessing [no PCA]')\n",
    "\n",
    "def run_data_ingestion_and_processing(raw_filename: str, clean_filename: str):\n",
    "    # 1) Download the raw file\n",
    "    ds = DataSourceClient().get_datasource(\"credit_card_fraud_detection\")\n",
    "    buf = io.BytesIO()\n",
    "    ds.download_fileobj(raw_filename, buf)\n",
    "    buf.seek(0)\n",
    "    df = pd.read_csv(buf)\n",
    "    print(f\"üîç Loaded {len(df):,} rows from {raw_filename}\")\n",
    "\n",
    "    # 2) Drop missing rows\n",
    "    before = len(df)\n",
    "    df = df.dropna()\n",
    "    after = len(df)\n",
    "    pct_removed = 100 * (before - after) / before if before > 0 else 0\n",
    "    print(f\"üßπ Dropped {before - after:,} rows with missing data\")\n",
    "\n",
    "    # 3) Match run_all: drop Class, Time, Hour from X\n",
    "    X = df.drop(columns=[\"Class\", \"Time\", \"Hour\"], errors=\"ignore\")\n",
    "    y = df[\"Class\"]\n",
    "\n",
    "    # 4) Detect numeric and categorical columns as in run_all\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=[object, \"category\"]).columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features,)\n",
    "    ])\n",
    "    pipeline = Pipeline([\n",
    "        (\"preproc\", preprocessor)\n",
    "    ])\n",
    "    start_time = time.time()\n",
    "    X_processed = pipeline.fit_transform(X)\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    # 5) Save processed data exactly as in run_all\n",
    "    np.save(f\"{domino_datasource_dir}/{domino_project_name}/X_processed.npy\", X_processed)\n",
    "    y.to_csv(f\"{domino_datasource_dir}/{domino_project_name}/y.csv\", index=False)\n",
    "    print(f\"‚úÖ Saved X_processed.npy and y.csv for downstream modeling\")\n",
    "\n",
    "    if hasattr(X_processed, \"toarray\"):\n",
    "        X_arr = X_processed.toarray()\n",
    "    else:\n",
    "        X_arr = X_processed\n",
    "    # get numeric names + one-hot names\n",
    "    num_cols = numeric_features\n",
    "    cat_cols = pipeline.named_steps[\"preproc\"] \\\n",
    "                       .named_transformers_[\"cat\"] \\\n",
    "                       .get_feature_names_out(categorical_features).tolist()\n",
    "    all_cols = num_cols + cat_cols\n",
    "\n",
    "    df_scaled = pd.DataFrame(X_arr, columns=all_cols)\n",
    "    df_scaled[\"Class\"] = y.values  # add back target if you like\n",
    "\n",
    "    # 2) Save it under clean_filename\n",
    "    clean_path = f\"{domino_datasource_dir}/{domino_project_name}/{clean_filename}\"\n",
    "    os.makedirs(os.path.dirname(clean_path), exist_ok=True)\n",
    "    df_scaled.to_csv(clean_path, index=False)\n",
    "\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # 6) Start MLflow run and log everything\n",
    "    with mlflow.start_run(run_name=\"Preprocessing Pipeline\") as run:\n",
    "        # Log parameters\n",
    "        mlflow.log_artifact(clean_path, artifact_path=\"data\")\n",
    "        mlflow.log_param(\"raw_filename\", raw_filename)\n",
    "        mlflow.log_param(\"clean_filename\", clean_filename)\n",
    "        mlflow.log_param(\"num_rows_loaded\", before)\n",
    "        mlflow.log_param(\"num_rows_after_dropna\", after)\n",
    "        mlflow.log_param(\"num_cat_features\", len(categorical_features))\n",
    "        mlflow.log_param(\"num_num_features\", len(numeric_features))\n",
    "\n",
    "        # Log human-readable pipeline parameters as YAML\n",
    "        pipeline_params = {\n",
    "            \"raw_filename\": raw_filename,\n",
    "            \"clean_filename\": clean_filename,\n",
    "            \"num_rows_loaded\": before,\n",
    "            \"num_rows_after_dropna\": after,\n",
    "            \"num_cat_features\": len(categorical_features),\n",
    "            \"num_num_features\": len(numeric_features),\n",
    "            \"categorical_columns\": categorical_features,\n",
    "            \"numerical_columns\": numeric_features,\n",
    "        }\n",
    "        params_yaml_path = f\"{domino_artifact_dir}/pipeline_params.yaml\"\n",
    "        with open(params_yaml_path, \"w\") as f:\n",
    "            yaml.dump(pipeline_params, f, default_flow_style=False)\n",
    "        mlflow.log_artifact(params_yaml_path, artifact_path=\"params\")\n",
    "\n",
    "        # Log the pipeline as a single model\n",
    "        X_sig = X.copy()\n",
    "        for col in numeric_features:\n",
    "            if np.issubdtype(X_sig[col].dtype, np.integer):\n",
    "                X_sig[col] = X_sig[col].astype(\"float64\")\n",
    "        signature = infer_signature(X_sig.iloc[:5], pipeline.transform(X_sig.iloc[:5]))\n",
    "        mlflow.sklearn.log_model(\n",
    "            pipeline,\n",
    "            artifact_path=\"preprocessing_pipeline\",\n",
    "            registered_model_name=\"CC Fraud Preprocessing\",\n",
    "            signature=signature\n",
    "        )\n",
    "        mlflow.set_tag(\"pipeline\", \"full_preproc_no_pca\")\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"pct_data_removed\", pct_removed)\n",
    "        mlflow.log_metric(\"num_rows_removed\", before - after)\n",
    "        mlflow.log_metric(\"preproc_fit_time_sec\", fit_time)\n",
    "\n",
    "        # 7) Generate and log artifacts (corr, scatter, etc.)\n",
    "        num_df = df.select_dtypes(include=\"number\").drop(columns=[\"Time\", \"Class\"], errors=\"ignore\")\n",
    "        # Correlation heatmap\n",
    "        plt.figure(figsize=(14,12))\n",
    "        sns.heatmap(num_df.corr(), annot=True, fmt=\".2f\", cmap=\"vlag\")\n",
    "        plt.title(\"Correlation Matrix\")\n",
    "        corr_path = f\"{domino_artifact_dir}/raw_correlation_matrix.png\"\n",
    "        plt.savefig(corr_path); plt.close()\n",
    "        mlflow.log_artifact(corr_path, artifact_path=\"plots\")\n",
    "        # Scatter matrix\n",
    "        sample_df = num_df.sample(n=500, random_state=0)\n",
    "        fig = scatter_matrix(sample_df, alpha=0.2, diagonal=\"hist\", figsize=(15,15))\n",
    "        scatter_path = f\"{domino_artifact_dir}/raw_scatter_plots.png\"\n",
    "        plt.savefig(scatter_path); plt.close()\n",
    "        mlflow.log_artifact(scatter_path, artifact_path=\"plots\")\n",
    "\n",
    "        # 8) EDA HTML\n",
    "        profile = ProfileReport(df, title=\"EDA Report\", explorative=True, minimal=True)\n",
    "        eda_path = f\"{domino_artifact_dir}/eda_report.html\"\n",
    "        profile.to_file(eda_path)\n",
    "        mlflow.log_artifact(eda_path, artifact_path=\"eda\")\n",
    "\n",
    "    return df, X_processed, y\n",
    "\n",
    "# Usage:\n",
    "raw_df, X_processed, y = run_data_ingestion_and_processing(\n",
    "    raw_filename=\"raw_cc_transactions.csv\",\n",
    "    clean_filename=\"cleaned_cc_transactions.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a70c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
